## 统计学习方法

### 解决问题

由于晶体的周期性质，我们寻找到的晶面也必然呈现周期的结构，如下图。而我们往往希望获取其中的一层原子来进行能量计算，因为我们这里创造性的运用统计学中的聚类方法，来从大量的原子层中分理处单个层的原子。

### K-Means

K-Means聚类算法是最为经典的，同时也是使用最为广泛的一种基于划分的聚类算法，它属于基于距离的聚类算法。所谓的基于距离的聚类算法指采用距离作为相似性度量的评价指标，也就是说，当两个对象离的近，二者之间的距离比较小，那么它们之间的相似性就比较大。这类算法通常是由距离比较相近的对象组成簇，把得到紧凑而且独立的簇作为最终目标，因此，将这类算法称为基于距离的聚类算法。  

K-Means聚类算法就是其中比较经典的一种算法。K-Means聚类是数据挖掘的重要分支，同时也是实际应用中最常用的聚类算法之一。本章重点是对K-Means聚类算法中的初始中心随机选取进行了分析与研究，给出了K-Means算法的思想和原理，优缺点的介绍以及现有的关于初始聚类中心选取的改进措施。  

1967年，J.B,MacQueen提出的K-Means算法是目前为止在工业和科学应用中一种极有影响的聚类技术。K-Means聚类算法是一种常用的基于划分的聚类分析方法，该聚类算法的最终目标就是根据输入参数k(这里的k表示需要将数据对象聚成几簇)，然后把数据对象分成k个簇。该算法的基本思想：首先指定需要划分的簇的个数k值;然后随机地选择k个初始数据对象点作为初始的聚类中心;第三，计算其余的各个数据对象到这k个初始聚类中心的距离(这里一般采用距离作为相似性度量)，把数据对象划归到距离它最近的那个中心所处在的簇类中;最后，调整新类并且重新计算出新类的中心，如果两次计算出来的聚类中心未曾发生任何的变化，那么就可以说明数据对象的调整己经结束，也就是说聚类采用的准则函数是收敛的，表示算法结束(这里采用的是误差平方和的准则函数)。  

K-Means聚类算法属于一种动态聚类算法，也称作逐步聚类法，该算法的一个比较显著的特点就是迭代过程，每次都要考察对每个样本数据的分类正确与否，如果不正确，就要进行调整。当调整完全部的数据对象之后，再来修改中心，最后进入下一次迭代的过程中。若在一个迭代中，所有的数据对象都己经被正确的分类，那么就不会有调整，聚类中心也不会改变，聚类准则函数也表明已经收敛，那么该算法就成功结束。  传统的K-Means算法的基本工作过程:首先随机选择k个数据作为初始中心，计算各个数据到所选出来的各个中心的距离，将数据对象指派到最近的簇中;然后计算每个簇的均值，循环往复执行，直到满足聚类准则函数收敛为止。通常采用的是平方误差准则函数，这个准则函数试图使生成的k个结果簇尽可能的紧凑和独立。其具体的工作步骤如下:  

```
输入: 初始数据集DATA和簇的数目k  
输出:k个簇，满足平方误差准则函数收敛  

1)任意选择k个数据对象作为初始聚类中心  
2)Repeat   
3)根据簇中对象的平均值，将每个对象赋给最类似的簇  
4)更新簇的平均值，即计算每个对象簇中对象的平均值  
5)计算聚类准则函数E   
6)Until准则函数E值不再进行变化
```

### k_means++

k-means算法是一种基本的聚类算法，这个算法的先决条件是

1）必须选择最终结果需要聚为几类，就是k的大小。  
2）初始化聚类中心点，也就是seeds。

当然，我们可以在输入的数据集中随机的选择k个点作为seeds，但是随机选择初始seeds可能会造成聚类的结果和数据的实际分布相差很大。既然选择初始的seeds这么重要，那有什么算法可以帮助选择初始的seeds吗？当然有，k-means++就是选择初始seeds的一种算法。

k-means++算法选择初始seeds的基本思想就是：初始的聚类中心之间的相互距离要尽可能的远。  

wiki上对该算法的描述是如下:  

从输入的数据点集合中随机选择一个点作为第一个聚类中心
对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x)
选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大
重复2和3直到k个聚类中心被选出来
利用这k个初始的聚类中心来运行标准的k-means算法
从上面的算法描述上可以看到，算法的关键是第3步，如何将D(x)反映到点被选择的概率上，一种算法如下(详见此地)：

先从我们的数据库随机挑个随机点当“种子点”
对于每个点，我们都计算其和最近的一个“种子点”的距离D(x)并保存在一个数组里，然后把这些距离加起来得到Sum(D(x))。
然后，再取一个随机值，用权重的方式来取计算下一个“种子点”。这个算法的实现是，先取一个能落在Sum(D(x))中的随机值Random，然后用Random -= D(x)，直到其<=0，此时的点就是下一个“种子点”。
重复2和3直到k个聚类中心被选出来
利用这k个初始的聚类中心来运行标准的k-means算法
可以看到算法的第三步选取新中心的方法，这样就能保证距离D(x)较大的点，会被选出来作为聚类中心了。

### 尝试其他聚类算法

* Affinity Propagation(AP)算法

跟其他聚类算法的不同之处是，AP在开始时，将所有节点都看成潜在的聚类中心，然后通过节点之间的通信，去找出最合适的聚类中心，并将其他节点划分到这些中心下去，所以我们可以认为，AP算法所要做的事情就是去发现这些聚类中心。

AP的输入是一个节点间的相似度矩阵，S，其中S(i,j)表示节点i和节点j之间的相似度，也表明了，j作为i的聚类中心的合适程度，这个相似度的计算可以根据具体应用场景，这里未免误导不作相似度的假设。其中S(k,k)表示节点k作为k的聚类中心的合适程度，可以理解为，节点k成为聚类中心合适度，在最开始时，这个值是初始化的时候使用者给定的值，会影响到最后聚类的数量。

AP中节点间传递的消息为两类：吸引度和归属度。

首先，吸引度是节点i传递向节点k的信息，传达了节点k对节点i的吸引度，记为r(i,k)，那么如何来衡量这个吸引度，其实吸引度是一个相对的概念，先前我们有相似度矩阵记录了k成为i的聚类中心的合适程度，那么这里我们只需要证明k比其他节点更合适了就可以了，那么其他节点是否合适这个如何进行衡量呢，是否合适其实就是看这两个节点是否相互认可，对于其他节点k'我们有s(i,k')表示节点k'作为节点i的聚类中心的合适度，那么再定义一个a(i,k')表示i对节点k'的认可程度（归属度），这两个值相加，a(i,k') + s(i,k')，就可以计算出节点k'作为节点i的聚类中心的合适程度了，这里，在所有其他节点k'中，找出最大的a(i,k') + s(i,k')，即max{a(i,k’)+s(i,k')}，再使用s(i,k) - max{a(i,k’)+s(i,k')} 就可以得出k对i的吸引度了，也就是第一个公式：

r(i,k) = s(i,k) - max{a(i,k’)+s(i,k')} 其中k != k'

接下来计算上面提到的归属度a(i,k)，表示了节点i选择节点k作为它的聚类中心的合适程度，这里要考虑到的一个思想是：如果节点k作为其他节点i'的聚类中心的合适度很大，那么节点k作为节点i的聚类中心的合适度也可能会较大，由此就可以先计算节点k对其他节点的吸引度，r(i',k)，然后做一个累加和表示节点k对其他节点的吸引度：

∑max{0，r(i',k)}

 ps.这里在r(i',k)跟0之间取一个大的原因是因为s(i',k)一般会初始化成负值，导致r(i',k)计算出来也有可能是负值，这样的好处是，最后可以方便找出合适的聚类中心在完成所有计算后。

然后再加上r(k,k)，这里为什么要加上r(k,k)，根据吸引度公式，我们可以看出，其实r(k,k)，反应的是节点k有多不适合被划分到其他聚类中心下去，这里的公式中，将k有多适合成为其他节点的聚类中心：∑max{0，r(i',k)}加上它有多不适合被划分到其他聚类中心下去：r(k,k) 就有了计算公式：

a(i,k)=min{0，r(k,k)+∑max{0，r(i',k)}}

 为了不让这个值过大，影响整体结果，将这个值控制在0以下。

其中a(k,k)的定义稍微有些不一样，只用∑max{0，r(i',k)}就可以了，主要反映k作为聚类中心的能力。

* DBSCAN

DBSCAN(Density-Based Spatial Clustering of Applications with Noise)是一个比较有代表性的基于密度的聚类算法。与划分和层次聚类方法不同，它将簇定义为密度相连的点的最大集合，能够把具有足够高密度的区域划分为簇，并可在噪声的空间数据库中发现任意形状的聚类。

DBSCAN中的几个定义：  
```
Ε邻域：给定对象半径为Ε内的区域称为该对象的Ε邻域；  
核心对象：如果给定对象Ε领域内的样本点数大于等于MinPts，则称该对象为核心对象；  
直接密度可达：对于样本集合D，如果样本点q在p的Ε领域内，并且p为核心对象，那么对象q从对象p直接密度可达。  
密度可达：对于样本集合D，给定一串样本点p1,p2….pn，p= p1,q= pn,假如对象pi从pi-1直接密度可达，那么对象q从对象p密度可达。  
密度相连：存在样本集合D中的一点o，如果对象o到对象p和对象q都是密度可达的，那么p和q密度相联。  
可以发现，密度可达是直接密度可达的传递闭包，并且这种关系是非对称的。密度相连是对称关系。DBSCAN目的是找到密度相连对象的最大集合。  
Eg: 假设半径Ε=3，MinPts=3，点p的E领域中有点{m,p,p1,p2,o}, 点m的E领域中有点{m,q,p,m1,m2},点q的E领域中有点{q,m},点o的E领域中有点{o,p,s},点s的E领域中有点{o,s,s1}.  
那么核心对象有p,m,o,s(q不是核心对象，因为它对应的E领域中点数量等于2，小于MinPts=3)；  
点m从点p直接密度可达，因为m在p的E领域内，并且p为核心对象；  
点q从点p密度可达，因为点q从点m直接密度可达，并且点m从点p直接密度可达；  
点q到点s密度相连，因为点q从点p密度可达，并且s从点p密度可达。
```

DBSCAN算法描述:  
```
输入: 包含n个对象的数据库，半径e，最少数目MinPts;  
输出:所有生成的簇，达到密度要求。  
(1)Repeat  
(2)从数据库中抽出一个未处理的点；  
(3)IF抽出的点是核心点 THEN 找出所有从该点密度可达的对象，形成一个簇；  
(4)ELSE 抽出的点是边缘点(非核心对象)，跳出本次循环，寻找下一个点；  
(5)UNTIL 所有的点都被处理。  
DBSCAN对用户定义的参数很敏感，细微的不同都可能导致差别很大的结果，而参数的选择无规律可循，只能靠经验确定。
```

* MeanShift

Mean Shift 这个概念最早是由Fukunaga等人于1975年在一篇关于概率密度梯度函数的估计（The Estimation of the Gradient of a Density Function, with Applications in Pattern Recognition ）中提出来的,其最初含义正如其名,就是偏移的均值向量,在这里Mean Shift是一个名词,它指代的是一个向量,但随着Mean Shift理论的发展,Mean Shift的含义也发生了变化,如果我们说Mean Shift算法,一般是指一个迭代的步骤,即先算出当前点的偏移均值,移动该点到其偏移均值,然后以此为新的起始点,继续移动,直到满足一定的条件结束.

然而在以后的很长一段时间内Mean Shift并没有引起人们的注意,直到20年以后,也就是1995年,另外一篇关于Mean Shift的重要文献（Mean shift, mode seeking, and clustering ）才发表.在这篇重要的文献中,Yizong Cheng对基本的Mean Shift算法在以下两个方面做了推广,首先Yizong Cheng定义了一族核函数,使得随着样本与被偏移点的距离不同,其偏移量对均值偏移向量的贡献也不同,其次Yizong Cheng还设定了一个权重系数,使得不同的样本点重要性不一样,这大大扩大了Mean Shift的适用范围.另外Yizong Cheng指出了Mean Shift可能应用的领域,并给出了具体的例子。

Comaniciu等人在还（Mean-shift Blob Tracking through Scale Space）中把非刚体的跟踪问题近似为一个Mean Shift最优化问题,使得跟踪可以实时的进行。目前，利用Mean Shift进行跟踪已经相当成熟。
